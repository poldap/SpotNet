{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition and testing of the SpotNet structure\n",
    "\n",
    "In this code section, we define a network that initially implements a number of iterations of the accelerated proximal gradient algorithm for detecting spots in FluoroSpot. From there on, we will train the network to get closer to the optimal than the actual number of iterations of the algorithm. For pictorical reference, the structure of the network extracted from the algorithm is shown below.\n",
    "\n",
    "[![network picture][network]](../../paper/figs/network.pdf)\n",
    "\n",
    "[network]: ../../paper/figs/network.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries to build and initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow for differentiable programming\n",
    "import tensorflow as tf\n",
    "# Numpy for array management\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library of non-linear functions $\\varphi_\\lambda(\\cdot)$\n",
    "\n",
    "These are linked to specific regularizers through proximal operators, in the sense that in general $\\varphi_\\lambda(x) = \\mbox{prox}_{\\lambda\\mathcal{R}}(x)$, where $\\mathcal{R}(x)$ is a regularizer that promotes certain characteristics of $x$, while \n",
    "$$\n",
    "    \\mbox{prox}_{\\lambda\\mathcal{R}}(x) = \\arg \\min_y \\left\\lbrace \\mathcal{R}(x) + \\frac{1}{2\\lambda}\\left\\|y-x\\right\\| \\right\\rbrace\\,  \\,\\,(1)\n",
    "$$\n",
    "\n",
    "Here, we include the cases \n",
    "$$\n",
    "\\mathcal{R}(x)= \\mathcal{R}_\\mathrm{s}(x) = \\sum_{k=1}^{K} \\left\\| x_k \\right\\|_1 = \\sum_{m,n,k}^{M,N,K} |x_{m,n,k}| \\, \\,\\,(2)\n",
    "$$\n",
    "and $\\mathcal{R}(x)= \\mathcal{R}_\\mathrm{s}(x) + \\delta_{\\mathbb{R}^{M,N,K}_+}(x)\\, \\,\\,(3)$ to promote sparsity and non-negative sparsity, respectively, but also\n",
    "$$\n",
    "\\mathcal{R}(x)= \\mathcal{R}_\\mathrm{gs}(x) = \\sum_{g=1}^{G} \\sqrt{ \\sum_{(i,j,k)\\in \\mathcal{G}_g}\\!\\!\\!\\!\\! x_k^2[i,j] } \\, \\,\\,(4)\n",
    "$$\n",
    "and\n",
    "$\\mathcal{R}(x)= \\mathcal{R}_\\mathrm{gs}(x) + \\delta_{\\mathbb{R}^{M,N,K}_+}(x)\\, \\,\\,(5)$ to promote group-sparsity and non-negative group-sparsity, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prox of Sparsity (l1 norm, (2))\n",
    "def soft_threshold( x, lam = 1.0 ):\n",
    "    b = tf.Variable( lam )\n",
    "    return (tf.nn.relu( x + b ) + tf.nn.relu( -x-b )) \n",
    "# Prox of Group sparsity (l2 norm along groups summed for all groups, (4)) \n",
    "# NWHC format with the same location in different channels belonging to the same group\n",
    "def soft_group_threshold( x, lam = 1.0 ):\n",
    "    b = tf.Variable(  lam )\n",
    "    norm = tf.reduce_sum( x**2, 3 )\n",
    "    return tf.expand_dims( tf.maximum( 1 - b / norm, 0 ) , axis = 3 ) * x\n",
    "# Prox of Non-negative Sparsity (l1 norm + non-negative infinity-0 indicator, (3))\n",
    "def s_nonneg( x, lam = 1.0 ):\n",
    "    b = tf.Variable( lam )\n",
    "    return tf.nn.relu( x - b )\n",
    "# Prox of Non-negative Group Sparsity \n",
    "# (l2 norm along groups summed for all groups + non-negative infinity-0 indicator, (5))\n",
    "def gs_nonneg( x ):\n",
    "    return soft_group_threshold( tf.maximum( x, 0 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial kernel generation\n",
    "\n",
    "In generic neural networks, one would generate the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random kernels corresponding to random features\n",
    "def generate_random_kernels( kernel_size, nrof_kernels ):\n",
    "        \n",
    "    kernels = tf.Variable( tf.truncated_normal( [kernel_size,\n",
    "                                                 kernel_size,\n",
    "                                                 nrof_kernels,\n",
    "                                                 1], stddev = 0.01 ) )\n",
    "\n",
    "    return kernels\n",
    "\n",
    "# Generate kernels given by the accelerated proximal gradient for spot detection in FluoroSpot\n",
    "# Import kernel generation from corresponding python module in our local folders\n",
    "import ker\n",
    "sigma_lims = np.array(\n",
    "                     [ 0.        ,  2.03803742,  4.07607485,  6.44484021,  8.64666049,\n",
    "                       10.78428037, 12.88968043, 14.97645529, 17.17280857, 19.33452064,\n",
    "                       21.47205663, 23.59198881, 25.77936086, 27.94416127, 30.09126194,\n",
    "                       32.22420107, 34.34561714, 36.5144422 , 38.66904129, 40.8116676 ,\n",
    "                       42.94411325, 45.1138815 , 47.27192547, 49.41978111, 51.55872171,\n",
    "                       53.68981279, 55.8511504 , 58.00356193, 60.14800566, 62.28530458,\n",
    "                       64.44840214 ] )\n",
    "\n",
    "generate_diffusion_kernels = lambda: ker.obtain_discrete_kernels( sigma_lims )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer( x_prev, z_prev, source, kernels, non_linearity = soft_threshold ):\n",
    "    \n",
    "    h = tf.Variable( kernels )\n",
    "    conv = tf.nn.depthwise_conv2d( input = z_prev,\n",
    "                                   filter = h,\n",
    "                                   strides = [1, 1, 1, 1],\n",
    "                                   padding = 'SAME' )\n",
    "    \n",
    "    u = tf.reduce_sum( conv, 3 ) - source\n",
    "    u = tf.expand_dims( u, 3 )\n",
    "    \n",
    "    h_m = tf.transpose( tf.reverse( tf.reverse( h, [0] ), [1] ), [0, 1, 3, 2] )\n",
    "\n",
    "    conv_m = tf.nn.conv2d( input = u,\n",
    "                           filter = h_m,\n",
    "                           strides = [1, 1, 1, 1],\n",
    "                           padding = 'SAME' )\n",
    "\n",
    "    x_current = non_linearity( z_prev - conv_m )\n",
    "    \n",
    "    alpha = tf.Variable( tf.constant( 0.5 ) )\n",
    "    z_current = x_current + alpha * (x_current - x_prev)\n",
    "        \n",
    "    return (x_current, z_current, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the SpotNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spot_finding_net( nrof_images,\n",
    "                      image_height, \n",
    "                      image_width, \n",
    "                      kernels,\n",
    "                      nrof_hidden_layers,\n",
    "                      non_linearity = soft_threshold ):\n",
    "    \n",
    "    # Seed for random initializations\n",
    "    tf.set_random_seed(8888)\n",
    "    \n",
    "    # Placeholder for final FluoroSpot images\n",
    "    source = tf.placeholder( tf.float32, [ nrof_images, image_height, image_width ] )\n",
    "    \n",
    "    # Create the input half-layer (equivalent to first iteration with initialization of 0)\n",
    "    h_m = tf.transpose( tf.reverse( tf.reverse( kernels, [0] ), [1] ), [0, 1, 3, 2] )\n",
    "    x = [ non_linearity( tf.nn.conv2d( input = tf.expand_dims( source, 3 ),\n",
    "                                       filter = h_m,\n",
    "                                       strides = [1, 1, 1, 1],\n",
    "                                       padding = 'SAME' ) ) ]\n",
    "    \n",
    "    alpha = tf.Variable( tf.constant( 0.5 ) )\n",
    "    z = [ (1 + alpha) * x[-1] ]\n",
    "    h = []\n",
    "    \n",
    "    # Create hidden layers as a feedforward convolutional graph. Append each hidden \n",
    "    # layer to a list and connect next layer to the last element of this list.\n",
    "    for i in range( nrof_hidden_layers ):\n",
    "        x_current, z_current, h_current = hidden_layer( x[-1], z[-1], source, kernels, non_linearity )\n",
    "        \n",
    "        x.append(x_current)\n",
    "        z.append(z_current)\n",
    "        h.append(h_current)\n",
    "        \n",
    "    # The output is simply the last x\n",
    "    output = x[-1]\n",
    "    \n",
    "    # Placeholder for PSDRs\n",
    "    target = tf.placeholder( tf.float32, [ nrof_images, image_height, image_width, nrof_kernels ] )\n",
    "    \n",
    "    # Mean squared error for the prediction of the PSDRs\n",
    "    loss = tf.reduce_mean( ( output - target ) ** 2 )\n",
    "    \n",
    "    return (source, loss, target, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of training strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return the optimizer\n",
    "def network_training( loss, learning_rate ):\n",
    "    #return tf.train.GradientDescentOptimizer( learning_rate ).minimize( loss )\n",
    "    return tf.train.AdamOptimizer( learning_rate ).minimize( loss )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluorospot data-------------------------------------------\n",
    "# Network parameters\n",
    "nrof_hidden_layers = 3\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1\n",
    "nrof_train_steps = 400000\n",
    "nrof_cells = 1250\n",
    "\n",
    "results_dir = 'spotnet_results/'\n",
    "prefix = 'SPOTNET_%d_CELLS_%d_LAYERS_%0.4f_LR_%d_STEPS_%d_BATCHSIZE'%( nrof_cells,\n",
    "                                                                       nrof_hidden_layers, \n",
    "                                                                       learning_rate,\n",
    "                                                                       nrof_train_steps,\n",
    "                                                                       batch_size )\n",
    "\n",
    "# Load dataset\n",
    "data = np.load( '../../sim_data/result_' + str(nrof_cells) + '_cells_10_images.npy' )[()]\n",
    "\n",
    "nrof_cells = data['nrof_cells']\n",
    "\n",
    "# Extract images and PSDRs\n",
    "images = data['fluorospot']\n",
    "psdrs = data['psdrs']\n",
    "\n",
    "# Extract shape parameters\n",
    "nrof_images, image_height, image_width = images.shape\n",
    "_, _, _, nrof_kernels = psdrs.shape\n",
    "\n",
    "# Split dataset for training and testing\n",
    "nrof_training_samples = int( 0.7 * nrof_images )\n",
    "train_images, train_psdrs = (images[ : nrof_training_samples, ... ], psdrs[ : nrof_training_samples, ... ])\n",
    "test_images, test_psdrs = (images[nrof_training_samples : , ... ], psdrs[nrof_training_samples : , ... ])\n",
    "\n",
    "print( 'Dataset: Num images: %d, Image height: %d, Image width: %d, Num cells: %d, Num kernels: %d'%( nrof_images, \n",
    "                                                                                                      image_height, \n",
    "                                                                                                      image_width,\n",
    "                                                                                                      nrof_cells,\n",
    "                                                                                                      nrof_kernels ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network computational graph\n",
    "\n",
    "# Extend to placeholder for kernels in future version to allow for different kernel sizes\n",
    "# kernels = tf.expand_dims( tf.Variable( generate_diffusion_kernels( ) ), 3 )\n",
    "\n",
    "# Smaller kernels (for testing)\n",
    "kernels = generate_random_kernels(5, 30) \n",
    "\n",
    "with tf.name_scope( 'SpotNet' ) as scope:\n",
    "    source, loss, target, output = spot_finding_net( batch_size,\n",
    "                                                     image_height, \n",
    "                                                     image_width, \n",
    "                                                     kernels,\n",
    "                                                     nrof_hidden_layers,\n",
    "                                                     soft_threshold )\n",
    "\n",
    "# Build training computational graph\n",
    "with tf.name_scope( 'Training' ) as scope:\n",
    "    train_step = network_training( loss, learning_rate )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrof_gpu = 1 # {0, 1}\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': nrof_gpu} )\n",
    "\n",
    "sess = tf.Session( config = config )\n",
    "sess.run( tf.global_variables_initializer( ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run session and training, see loss evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print( 'Training the network' )\n",
    "\n",
    "# Space to store loss values\n",
    "train_loss_record = np.empty( (0, ) )\n",
    "test_loss_record  = np.empty( (0, ) )\n",
    "iterations_record = np.empty( (0, ) )\n",
    "\n",
    "# Indices that will be picked at random at each iteration\n",
    "indices = np.arange( train_images.shape[0] )\n",
    "np.random.seed(8888)\n",
    "\n",
    "# Train the network\n",
    "start = time.time()\n",
    "for training_iteration in range( nrof_train_steps + 1 ):\n",
    "    \n",
    "    \n",
    "    np.random.shuffle( indices )\n",
    "    batch_indices = np.take( indices, np.arange( batch_size ), mode = 'wrap' )\n",
    "    batch_input = train_images[ batch_indices, ... ]\n",
    "    batch_target = train_psdrs[ batch_indices, ... ]\n",
    "    \n",
    "    _, train_loss = sess.run( [ train_step, loss ], feed_dict = { source: batch_input, \n",
    "                                                                  target: batch_target } )\n",
    "    \n",
    "    \n",
    "    if training_iteration % (nrof_train_steps / 100) == 0:\n",
    "        test_loss = 0\n",
    "        for test_image_index in range( test_images.shape[0] ):\n",
    "            test_loss += sess.run( loss, feed_dict = { source: np.expand_dims( test_images[ test_image_index, ... ], axis = 0 ), \n",
    "                                                       target: np.expand_dims( test_psdrs[  test_image_index, ... ], axis = 0 ) } )\n",
    "        test_loss = test_loss / test_images.shape[0]\n",
    "        test_loss_record = np.append( test_loss_record, test_loss )\n",
    "        train_loss_record = np.append( train_loss_record, train_loss )\n",
    "        iterations_record = np.append( iterations_record, training_iteration )\n",
    "        \n",
    "        if (test_loss == test_loss_record.min() and training_iteration > 10):\n",
    "            print( 'Train step %d, Batch loss: %0.4f, Test loss: %0.4f, Elapsed: %ds. Best in test yet! Storing.'%( \n",
    "                                                                                    training_iteration, \n",
    "                                                                                    train_loss, \n",
    "                                                                                    test_loss, \n",
    "                                                                                    time.time() - start ) ) \n",
    "            tf.saved_model.simple_save( sess,\n",
    "                results_dir + 'trained_spotnet_' + str( training_iteration ) + '_train-steps_5_kersize/',\n",
    "                inputs = {'image': source},\n",
    "                outputs = {'psdr': output} )\n",
    "        else:\n",
    "            print( 'Train step %d, Batch loss: %0.4f, Test loss: %0.4f, Elapsed: %ds.'%( \n",
    "                                                                                    training_iteration, \n",
    "                                                                                    train_loss, \n",
    "                                                                                    test_loss, \n",
    "                                                                                    time.time() - start ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.simple_save(\n",
    "    sess,\n",
    "    results_dir + 'trained_spotnet_' + str( nrof_train_steps) + '_train-steps/',\n",
    "    inputs = {'image': source},\n",
    "    outputs = {'psdr': output}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'spotnet_results/'\n",
    "results = np.loadtxt( results_dir + 'trained_spotnet_history_5_kersize.txt' )\n",
    "iterations_record = results[:,0]; train_loss_record = results[:,1]; test_loss_record = results[:,2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure( figsize = [20,20] );\n",
    "plt.plot(iterations_record,train_loss_record,iterations_record,test_loss_record);\n",
    "plt.legend( ( 'Train loss (Batch size = 1)', 'Test loss (3 images)' ) );\n",
    "plt.xlabel('Training steps');\n",
    "plt.ylabel(\"MSE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
